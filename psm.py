from playwright.sync_api import sync_playwright
import requests
import time
from datetime import datetime, timedelta
import random
import json
import os
import threading
from http.server import HTTPServer, SimpleHTTPRequestHandler
import locale
from flask import Flask, jsonify, request
from flask_cors import CORS
import collections

# ====================
# SYST√àME DE LOGS POUR L'ADMIN
# ====================
backend_logs = collections.deque(maxlen=200)

def log(message, log_type='info'):
    """Log un message dans la console ET dans backend_logs"""
    timestamp = datetime.now().strftime("%H:%M:%S")
    
    # Afficher dans la console
    print(message)
    
    # Stocker dans la liste
    backend_logs.append({
        'timestamp': timestamp,
        'type': log_type,
        'message': message
    })

# Configuration Playwright pour Docker
os.environ['PLAYWRIGHT_BROWSERS_PATH'] = os.environ.get('PLAYWRIGHT_BROWSERS_PATH', '/root/.cache/ms-playwright')

# Configuration locale pour les dates en fran√ßais
try:
    locale.setlocale(locale.LC_TIME, 'fr_FR.UTF-8')
except:
    try:
        locale.setlocale(locale.LC_TIME, 'French_France.1252')
    except:
        pass  # Si la locale n'est pas disponible, on utilisera une fonction de remplacement

# ====================
# HISTORIQUE DES D√âTECTIONS PMR
# ====================
DETECTIONS_HISTORY_FILE = 'detections_history.json'

def charger_historique_detections():
    """Charge l'historique des d√©tections PMR"""
    try:
        if os.path.exists(DETECTIONS_HISTORY_FILE):
            with open(DETECTIONS_HISTORY_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
    except Exception as e:
        log(f"‚ö†Ô∏è Erreur chargement historique: {e}", 'warning')
    return []

def sauvegarder_detection(match_nom, nb_places):
    """Sauvegarde une d√©tection PMR dans l'historique"""
    try:
        historique = charger_historique_detections()
        detection = {
            "match": match_nom,
            "nb_places": nb_places,
            "date": datetime.now().isoformat(),
            "date_formatee": formater_date_francaise(datetime.now())
        }
        historique.append(detection)
        # Garder seulement les 50 derni√®res d√©tections
        if len(historique) > 50:
            historique = historique[-50:]
        
        with open(DETECTIONS_HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump(historique, f, ensure_ascii=False, indent=2)
    except Exception as e:
        log(f"‚ö†Ô∏è Erreur sauvegarde d√©tection: {e}", 'warning')

# Charger les matchs depuis le fichier JSON
def charger_matchs():
    try:
        with open('matches.json', 'r', encoding='utf-8') as f:
            matches = json.load(f)
        log(f"üìÇ matches.json charg√©: {len(matches)} match(s)", 'info')
        return matches
    except FileNotFoundError:
        # Matchs par d√©faut si le fichier n'existe pas
        matchs_default = [
    {
        "nom": "PSG vs PARIS FC",
        "url": "https://billetterie.psg.fr/fr/catalogue/match-foot-masculin-paris-sg-vs-paris-fc-1"
    },
    {
        "nom": "PSG vs RENNE",
        "url": "https://billetterie.psg.fr/fr/catalogue/match-foot-masculin-paris-vs-rennes-5"
            }
        ]
        with open('matches.json', 'w', encoding='utf-8') as f:
            json.dump(matchs_default, f, ensure_ascii=False, indent=2)
        log(f"üìÇ matches.json cr√©√© avec {len(matchs_default)} match(s) par d√©faut", 'info')
        return matchs_default

# ====================
# FONCTIONS HELPER POUR GROQ
# ====================

def extract_teams_from_match_name(match_name):
    """Extrait les √©quipes depuis le nom du match"""
    # Format attendu: "PSG vs OM" ou "PSG vs PARIS FC"
    parts = match_name.split(' vs ')
    if len(parts) == 2:
        return {'home': parts[0].strip(), 'away': parts[1].strip()}
    return {'home': 'PSG', 'away': 'Adversaire'}

def detect_match_importance(home_team, away_team, match_name):
    """D√©tecte l'importance du match"""
    away_lower = away_team.lower()
    match_lower = match_name.lower()
    
    is_classico = 'classique' in match_lower or (home_team == 'PSG' and ('om' in away_lower or 'marseille' in away_lower))
    is_ol = 'lyon' in away_lower or 'ol' in away_lower
    is_monaco = 'monaco' in away_lower
    is_high_profile = is_classico or is_ol or is_monaco
    
    return {
        'is_classico': is_classico,
        'is_ol': is_ol,
        'is_monaco': is_monaco,
        'is_high_profile': is_high_profile,
        'rivalry': 'Le Classique' if is_classico else ('Grande affiche' if is_ol else ('Match attractif' if is_monaco else 'Match r√©gulier'))
    }

def get_comparison_matches(match_name, home_team, limit=3):
    """R√©cup√®re les VRAIS autres matchs depuis matches.json pour la comparaison"""
    try:
        matches_data = charger_matchs()  # Utiliser la fonction existante
        
        # Filtrer les matchs : m√™me √©quipe √† domicile, exclure le match actuel
        comparison_matches = []
        for match in matches_data:
            match_nom = match.get('nom', '')
            # V√©rifier que c'est un match √† domicile de la m√™me √©quipe
            if home_team in match_nom and match_nom != match_name:
                # Extraire l'√©quipe adverse
                parts = match_nom.split(' vs ')
                if len(parts) == 2 and parts[0].strip() == home_team:
                    away_team = parts[1].strip()
                    comparison_matches.append({
                        'name': match_nom,
                        'away_team': away_team,
                        'url': match.get('url', ''),
                        'key': f'match_{len(comparison_matches) + 1}'
                    })
        
        # Si pas assez de matchs r√©els, compl√©ter avec des matchs estim√©s
        if len(comparison_matches) < limit:
            fallback_matches = [
                {'name': f'{home_team} vs Lyon', 'key': 'match_fallback_1'},
                {'name': f'{home_team} vs Monaco', 'key': 'match_fallback_2'},
                {'name': f'{home_team} vs Lens', 'key': 'match_fallback_3'}
            ]
            # Exclure ceux qui sont d√©j√† dans comparison_matches
            for fallback in fallback_matches:
                if len(comparison_matches) >= limit:
                    break
                away_lower = fallback['name'].split(' vs ')[1].lower()
                if not any(away_lower in m['name'].lower() for m in comparison_matches):
                    if match_name.lower() not in fallback['name'].lower():
                        comparison_matches.append(fallback)
        
        return comparison_matches[:limit]
    except Exception as e:
        log(f"‚ö†Ô∏è Erreur r√©cup√©ration matchs de comparaison: {e}", 'warning')
        # Fallback avec matchs par d√©faut
        return [
            {'name': f'{home_team} vs Lyon', 'key': 'match_1'},
            {'name': f'{home_team} vs Monaco', 'key': 'match_2'},
            {'name': f'{home_team} vs Lens', 'key': 'match_3'}
        ]

# ====================
# SYST√àME DE CACHE GROQ
# ====================
GROQ_CACHE_FILE = 'groq_cache.json'

def get_cached_groq_data(match_name):
    """R√©cup√®re les donn√©es en cache si elles existent et sont r√©centes (< 24h)"""
    try:
        with open(GROQ_CACHE_FILE, 'r', encoding='utf-8') as f:
            cache = json.load(f)
        
        if match_name in cache:
            cached_data = cache[match_name]
            last_updated = datetime.fromisoformat(cached_data.get('last_updated', '2000-01-01'))
            hours_diff = (datetime.now() - last_updated).total_seconds() / 3600
            
            if hours_diff < 24:
                log(f"‚úÖ Donn√©es Groq en cache pour {match_name} ({hours_diff:.1f}h)", 'info')
                return cached_data
            else:
                log(f"‚è∞ Cache expir√© pour {match_name} ({hours_diff:.1f}h)", 'info')
    except FileNotFoundError:
        pass
    except Exception as e:
        log(f"‚ö†Ô∏è Erreur lecture cache: {e}", 'warning')
    
    return None

def save_groq_cache(match_name, data):
    """Sauvegarde les donn√©es dans le cache"""
    try:
        cache = {}
        if os.path.exists(GROQ_CACHE_FILE):
            with open(GROQ_CACHE_FILE, 'r', encoding='utf-8') as f:
                cache = json.load(f)
        
        cache[match_name] = data
        cache[match_name]['last_updated'] = datetime.now().isoformat()
        
        with open(GROQ_CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)
        
        log(f"üíæ Cache Groq sauvegard√© pour {match_name}", 'info')
    except Exception as e:
        log(f"‚ö†Ô∏è Erreur sauvegarde cache: {e}", 'warning')

# ‚úÖ LISTE DES MATCHS √Ä SURVEILLER (charg√©e dynamiquement)
MATCHS = charger_matchs()

TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN", "8222793392:AAFBtlCNAlPyUYgf1aup06HAvRO9V14DmRo")
CHAT_ID = os.getenv("TELEGRAM_CHAT_ID", "-1003428870741")

# Cooldown par match
dernier_message_indispo = {}

# Statistiques pour le status.json
nb_checks_par_match = {}
dernier_check_par_match = {}
pmr_disponible_par_match = {}

# Mois en fran√ßais
MOIS_FR = {
    1: "janvier", 2: "f√©vrier", 3: "mars", 4: "avril",
    5: "mai", 6: "juin", 7: "juillet", 8: "ao√ªt",
    9: "septembre", 10: "octobre", 11: "novembre", 12: "d√©cembre"
}

def formater_date_francaise(dt):
    """Formate une date en fran√ßais avec le mois en lettres"""
    jour = dt.day
    mois = MOIS_FR[dt.month]
    annee = dt.year
    heure = dt.strftime("%H:%M:%S")
    return f"{jour} {mois} {annee} √† {heure}"

def envoyer_message(msg):
    url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
    data = {"chat_id": CHAT_ID, "text": msg}
    try:
        r = requests.post(url, data=data, timeout=10)
        print("Telegram:", r.text)
    except Exception as e:
        print("Erreur Telegram:", e)

def sauvegarder_status():
    """Sauvegarde l'√©tat du bot dans status.json pour le site web"""
    status = {
        "bot_actif": True,
        "derniere_mise_a_jour": formater_date_francaise(datetime.now()),
        "matchs": []
    }
    
    total_checks = 0
    alertes_envoyees = 0
    
    for match in MATCHS:
        nom = match["nom"]
        
        # Calculer le temps depuis le dernier check
        if nom in dernier_check_par_match:
            dernier_check = dernier_check_par_match[nom]
            diff = datetime.now() - dernier_check
            minutes = int(diff.total_seconds() / 60)
            if minutes < 1:
                dernier_check_str = "√Ä l'instant"
            elif minutes == 1:
                dernier_check_str = "Il y a 1 min"
            else:
                dernier_check_str = f"Il y a {minutes} min"
        else:
            dernier_check_str = "En attente..."
        
        # R√©cup√©rer les statistiques
        nb_checks = nb_checks_par_match.get(nom, 0)
        pmr_dispo = pmr_disponible_par_match.get(nom, False)
        total_checks += nb_checks
        
        # Compter les alertes (quand PMR √©tait disponible)
        if pmr_dispo:
            alertes_envoyees += 1
        
        status["matchs"].append({
            "nom": nom,
            "url": match["url"],
            "pmr_disponible": pmr_dispo,
            "dernier_check": dernier_check_str,
            "nb_checks": nb_checks
        })
    
    # Calculer le taux de disponibilit√© (pourcentage de fois o√π PMR √©tait disponible)
    nb_matchs = len(MATCHS)
    if nb_matchs > 0:
        matchs_avec_pmr = sum(1 for match in MATCHS if pmr_disponible_par_match.get(match["nom"], False))
        taux_disponibilite = round((matchs_avec_pmr / nb_matchs) * 100, 1)
    else:
        taux_disponibilite = 0.0
    
    # Ajouter les statistiques globales
    status["statistiques"] = {
        "verifications_totales": total_checks,
        "alertes_envoyees": alertes_envoyees,
        "taux_disponibilite": f"{taux_disponibilite}%",
        "matchs_surveilles": nb_matchs
    }
    
    import os
    status_path = 'status.json'
    with open(status_path, 'w', encoding='utf-8') as f:
        json.dump(status, f, ensure_ascii=False, indent=2)
    print(f"üíæ status.json sauvegard√© dans: {os.path.abspath(status_path)}")

def verifier_match(match):
    nom = match["nom"]
    url = match["url"]

    if nom not in dernier_message_indispo:
        dernier_message_indispo[nom] = datetime.now() - timedelta(hours=8)

    try:
        with sync_playwright() as p:
            # Configuration pour Docker/headless
            browser = p.chromium.launch(
                headless=True,
                args=[
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-gpu',
                    '--disable-software-rasterizer',
                    '--disable-extensions',
                    '--window-size=1920x1080',
                    '--disable-blink-features=AutomationControlled',
                    '--disable-features=IsolateOrigins,site-per-process',
                    '--single-process',
                    '--no-zygote'
                ],
                timeout=60000
            )
            
            context = browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            )
            
            page = context.new_page()
            
            # Configuration des timeouts plus longs
            page.set_default_timeout(120000)  # 120 secondes pour toutes les op√©rations
            page.set_default_navigation_timeout(120000)

            log(f"üåê Chargement de {nom}...", 'info')
            try:
                page.goto(url, timeout=120000, wait_until="domcontentloaded")
                log(f"‚úÖ Page charg√©e pour {nom}", 'success')
            except Exception as goto_error:
                log(f"‚ö†Ô∏è Erreur lors du chargement de la page pour {nom}: {goto_error}", 'warning')
                log(f"üîÑ Nouvelle tentative...", 'info')
                page.goto(url, timeout=120000, wait_until="domcontentloaded")
                log(f"‚úÖ Page charg√©e pour {nom} (2√®me tentative)", 'success')
            
            # Attendre BEAUCOUP plus longtemps que le contenu se charge
            log(f"‚è≥ Attente du chargement complet...", 'info')
            page.wait_for_timeout(10000)  # 10 secondes au lieu de 4
            
            # Scroll AVANT de chercher les √©l√©ments
            log(f"üìú Scroll de la page...", 'info')
            for i in range(5):  # Plus de scrolls
                page.mouse.wheel(0, 1500)
                page.wait_for_timeout(2000)  # Plus de temps entre chaque scroll
            
            # Attendre encore apr√®s le scroll
            page.wait_for_timeout(5000)
            
            # Essayer de cliquer sur un bouton si pr√©sent (pour d√©clencher le chargement)
            try:
                page.wait_for_selector('button, .button, [role="button"]', timeout=5000)
            except:
                pass

            heure = datetime.now().strftime("%H:%M:%S")

            pmr_elements = page.query_selector_all('div[data-offer-type="PMR"]')
            log(f"{nom} ‚Üí PMR trouv√©es : {len(pmr_elements)}", 'info')
            
            # Sauvegarder la d√©tection si des PMR sont trouv√©es
            if len(pmr_elements) > 0:
                sauvegarder_detection(nom, len(pmr_elements))

            # Mettre √† jour les statistiques
            nb_checks_par_match[nom] = nb_checks_par_match.get(nom, 0) + 1
            dernier_check_par_match[nom] = datetime.now()
            pmr_disponible_par_match[nom] = len(pmr_elements) > 0

            if len(pmr_elements) > 0:
                envoyer_message(f"üî• ALERTE PLACE PMR DISPONIBLE ! üî•\n\nüéüÔ∏è Match : {nom}\n‚úÖ Places PMR trouv√©es !\n\nüëâ Fonce sur la billetterie maintenant !")
            else:
                if datetime.now() - dernier_message_indispo[nom] >= timedelta(hours=8):
                    envoyer_message(f"üò¥ Pas encore de places PMR...\n\nüéüÔ∏è Match : {nom}\n‚ùå Aucune place PMR disponible pour le moment\n\nüí™ On continue de surveiller pour toi !")
                    dernier_message_indispo[nom] = datetime.now()
                else:
                    log(f"{nom} ‚Üí Pas de PMR (cooldown actif)", 'info')

            # Sauvegarder le status avant de fermer
            sauvegarder_status()

            context.close()
            browser.close()

    except Exception as e:
        log(f"‚ö†Ô∏è Erreur sur {nom} : {e}", 'error')
        import traceback
        log(f"üìã D√©tails de l'erreur :", 'error')
        traceback.print_exc()
        # Sauvegarder le status m√™me en cas d'erreur
        sauvegarder_status()

# Cr√©er le fichier status.json initial
sauvegarder_status()

# ====================
# API FLASK
# ====================
app = Flask(__name__)
# D√©sactiver CORS dans Flask car le serveur web le g√®re d√©j√†
# Cela √©vite les conflits de headers CORS multiples
CORS(app, resources={r"/api/*": {"origins": "*", "supports_credentials": False}})

# Chemins des fichiers
MATCHES_FILE = 'matches.json'
ANALYTICS_FILE = 'analytics.json'

@app.route('/api/status', methods=['GET'])
def api_get_status():
    """Retourne le statut complet du bot depuis status.json"""
    try:
        with open('status.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
        return jsonify(data)
    except FileNotFoundError:
        return jsonify({"error": "Status file not found"}), 404
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/matches', methods=['GET'])
def api_get_matches():
    """Liste tous les matchs surveill√©s"""
    try:
        with open(MATCHES_FILE, 'r', encoding='utf-8') as f:
            matches = json.load(f)
        return jsonify(matches)
    except FileNotFoundError:
        # Si le fichier n'existe pas, le cr√©er avec les matchs par d√©faut
        default_matches = charger_matchs()
        return jsonify(default_matches)

@app.route('/api/matches', methods=['POST'])
def api_add_match():
    """Ajoute un nouveau match √† surveiller"""
    try:
        data = request.json
        nom = data.get('nom', '').strip()
        url = data.get('url', '').strip()
        
        # Validation
        if not nom or not url:
            return jsonify({"error": "Nom et URL requis"}), 400
        
        # Validation de l'URL
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            if not parsed.scheme or not parsed.netloc:
                return jsonify({"error": "URL invalide. Veuillez entrer une URL compl√®te (ex: https://...)"}), 400
        except Exception:
            return jsonify({"error": "URL invalide"}), 400
        
        # Lire les matchs existants
        try:
            with open(MATCHES_FILE, 'r', encoding='utf-8') as f:
                matches = json.load(f)
        except FileNotFoundError:
            matches = []
        
        # V√©rifier si le match existe d√©j√†
        for match in matches:
            if match.get('nom') == nom:
                return jsonify({"error": f"Un match avec le nom '{nom}' existe d√©j√†"}), 409
            if match.get('url') == url:
                return jsonify({"error": f"Un match avec cette URL existe d√©j√†"}), 409
        
        # Ajouter le nouveau match
        new_match = {"nom": nom, "url": url}
        matches.append(new_match)
        
        # Sauvegarder
        with open(MATCHES_FILE, 'w', encoding='utf-8') as f:
            json.dump(matches, f, ensure_ascii=False, indent=2)
        
        # Mettre √† jour status.json imm√©diatement
        global MATCHS
        MATCHS = matches  # Mettre √† jour la variable globale
        sauvegarder_status()  # Mettre √† jour status.json pour que le site l'affiche
        
        log(f"‚úÖ Match ajout√©: {nom} ({url})", 'success')
        log(f"üìä Total de matchs surveill√©s: {len(matches)}", 'info')
        log(f"üîÑ Le match sera v√©rifi√© au prochain cycle de surveillance (~90 secondes)", 'info')
        log(f"üíæ matches.json mis √† jour avec succ√®s", 'success')
        log(f"üíæ status.json mis √† jour - le nouveau match appara√Æt sur le site public", 'success')
        
        return jsonify({"success": True, "match": new_match}), 201
    except Exception as e:
        print(f"‚ùå Erreur ajout match: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500

@app.route('/api/matches/<int:index>', methods=['DELETE'])
def api_delete_match(index):
    """Supprime un match par son index"""
    try:
        with open(MATCHES_FILE, 'r', encoding='utf-8') as f:
            matches = json.load(f)
        
        if 0 <= index < len(matches):
            deleted = matches.pop(index)
            
            # Sauvegarder
            with open(MATCHES_FILE, 'w', encoding='utf-8') as f:
                json.dump(matches, f, ensure_ascii=False, indent=2)
            
            # Mettre √† jour status.json imm√©diatement
            global MATCHS
            MATCHS = matches  # Mettre √† jour la variable globale
            sauvegarder_status()  # Mettre √† jour status.json
            
            log(f"üóëÔ∏è Match supprim√©: {deleted.get('nom')} ({deleted.get('url')})", 'error')
            log(f"üìä Matchs restants: {len(matches)}", 'info')
            log(f"üíæ matches.json mis √† jour avec succ√®s", 'success')
            log(f"üíæ status.json mis √† jour - le site public refl√®te le changement", 'success')
            log(f"‚è∏Ô∏è Le match ne sera plus surveill√© au prochain cycle", 'info')
            
            return jsonify({"success": True, "deleted": deleted})
        else:
            return jsonify({"error": "Index invalide"}), 404
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/matches/<int:index>/check', methods=['POST'])
def api_force_check(index):
    """Force la v√©rification d'un match sp√©cifique"""
    try:
        # Charger les matchs
        try:
            with open(MATCHES_FILE, 'r', encoding='utf-8') as f:
                matches = json.load(f)
        except FileNotFoundError:
            matches = charger_matchs()
        
        # V√©rifier que l'index est valide
        if 0 <= index < len(matches):
            match = matches[index]
            nom = match.get("nom", "Match inconnu")
            
            # Lancer la v√©rification dans un thread s√©par√© pour ne pas bloquer
            def verifier_en_background():
                url_match = match.get("url", "URL inconnue")
                log(f"üîÑ V√©rification forc√©e de {nom}...", 'info')
                log(f"üåê URL: {url_match}", 'info')
                verifier_match(match)
                log(f"‚úÖ V√©rification forc√©e de {nom} termin√©e", 'success')
            
            threading.Thread(target=verifier_en_background, daemon=True).start()
            
            return jsonify({
                "success": True, 
                "message": f"V√©rification de {nom} lanc√©e en arri√®re-plan"
            })
        else:
            return jsonify({"error": "Index invalide"}), 404
    except Exception as e:
        print(f"‚ùå Erreur v√©rification forc√©e: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500

@app.route('/api/analytics', methods=['GET'])
def api_get_analytics():
    """Retourne les statistiques du site web"""
    try:
        with open(ANALYTICS_FILE, 'r', encoding='utf-8') as f:
            analytics = json.load(f)
        
        # S'assurer que toutes les propri√©t√©s existent
        default_values = {
            "visiteurs_totaux": 0,
            "visiteurs_en_ligne": 0,
            "visiteurs_aujourdhui": 0,
            "temps_moyen": "0m 0s",
            "taux_rebond": "0%",
            "clics_telegram": 0,
            "pic_connexions": 0,
            "taux_retour": "0%",
            "historique_7j": [0, 0, 0, 0, 0, 0, 0],
            "derniere_date": None
        }
        
        # Remplir les valeurs manquantes
        for key, default_value in default_values.items():
            if key not in analytics:
                analytics[key] = default_value
        
        # V√©rifier si l'historique doit √™tre mis √† jour (nouveau jour)
        date_actuelle = datetime.now().strftime("%Y-%m-%d")
        derniere_date = analytics.get("derniere_date")
        
        if derniere_date != date_actuelle and derniere_date is not None:
            # Nouveau jour d√©tect√©, mettre √† jour l'historique
            try:
                derniere_date_obj = datetime.strptime(derniere_date, "%Y-%m-%d")
                date_actuelle_obj = datetime.strptime(date_actuelle, "%Y-%m-%d")
                jours_ecoules = (date_actuelle_obj - derniere_date_obj).days
                
                if jours_ecoules > 0:
                    # D√©caler l'historique
                    for i in range(min(jours_ecoules, 7)):
                        analytics["historique_7j"].pop(0)
                        analytics["historique_7j"].append(0)
                    
                    if jours_ecoules >= 7:
                        analytics["historique_7j"] = [0, 0, 0, 0, 0, 0, 0]
                    
                    analytics["visiteurs_aujourdhui"] = 0
                    analytics["derniere_date"] = date_actuelle
                    
                    # Sauvegarder la mise √† jour
                    with open(ANALYTICS_FILE, 'w', encoding='utf-8') as f:
                        json.dump(analytics, f, ensure_ascii=False, indent=2)
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur mise √† jour historique: {e}")
        
        return jsonify(analytics)
    except FileNotFoundError:
        # Cr√©er des stats par d√©faut (valeurs r√©elles, pas de simulation)
        default_analytics = {
            "visiteurs_totaux": 0,
            "visiteurs_en_ligne": 0,
            "visiteurs_aujourdhui": 0,
            "temps_moyen": "0m 0s",
            "taux_rebond": "0%",
            "clics_telegram": 0,
            "pic_connexions": 0,
            "taux_retour": "0%",
            "historique_7j": [0, 0, 0, 0, 0, 0, 0],
            "derniere_date": datetime.now().strftime("%Y-%m-%d")
        }
        with open(ANALYTICS_FILE, 'w', encoding='utf-8') as f:
            json.dump(default_analytics, f, ensure_ascii=False, indent=2)
        return jsonify(default_analytics)
    except Exception as e:
        print(f"‚ùå Erreur lecture analytics: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500

@app.route('/api/analytics/visitor', methods=['POST'])
def api_track_visitor():
    """Enregistre une visite sur le site"""
    try:
        # Charger analytics
        try:
            with open(ANALYTICS_FILE, 'r', encoding='utf-8') as f:
                analytics = json.load(f)
        except FileNotFoundError:
            # Initialiser avec toutes les propri√©t√©s n√©cessaires
            analytics = {
                "visiteurs_totaux": 0,
                "visiteurs_en_ligne": 0,
                "visiteurs_aujourdhui": 0,
                "temps_moyen": "0m 0s",
                "taux_rebond": "0%",
                "clics_telegram": 0,
                "pic_connexions": 0,
                "taux_retour": "0%",
                "historique_7j": [0, 0, 0, 0, 0, 0, 0],
                "derniere_date": None
            }
        
        # S'assurer que toutes les propri√©t√©s existent
        if "visiteurs_totaux" not in analytics:
            analytics["visiteurs_totaux"] = 0
        if "visiteurs_en_ligne" not in analytics:
            analytics["visiteurs_en_ligne"] = 0
        if "visiteurs_aujourdhui" not in analytics:
            analytics["visiteurs_aujourdhui"] = 0
        if "temps_moyen" not in analytics:
            analytics["temps_moyen"] = "0m 0s"
        if "taux_rebond" not in analytics:
            analytics["taux_rebond"] = "0%"
        if "clics_telegram" not in analytics:
            analytics["clics_telegram"] = 0
        if "pic_connexions" not in analytics:
            analytics["pic_connexions"] = 0
        if "taux_retour" not in analytics:
            analytics["taux_retour"] = "0%"
        if "historique_7j" not in analytics:
            analytics["historique_7j"] = [0, 0, 0, 0, 0, 0, 0]
        if "derniere_date" not in analytics:
            analytics["derniere_date"] = None
        
        # Obtenir la date actuelle (format YYYY-MM-DD)
        date_actuelle = datetime.now().strftime("%Y-%m-%d")
        derniere_date = analytics.get("derniere_date")
        
        # Si c'est un nouveau jour, mettre √† jour l'historique
        if derniere_date != date_actuelle:
            if derniere_date is not None:
                # Calculer le nombre de jours √©coul√©s
                try:
                    derniere_date_obj = datetime.strptime(derniere_date, "%Y-%m-%d")
                    date_actuelle_obj = datetime.strptime(date_actuelle, "%Y-%m-%d")
                    jours_ecoules = (date_actuelle_obj - derniere_date_obj).days
                    
                    # Si plus d'un jour s'est √©coul√©, d√©caler l'historique
                    if jours_ecoules > 0:
                        # D√©caler l'historique vers la gauche
                        for i in range(min(jours_ecoules, 7)):
                            analytics["historique_7j"].pop(0)
                            analytics["historique_7j"].append(0)
                        
                        # Si plus de 7 jours, r√©initialiser
                        if jours_ecoules >= 7:
                            analytics["historique_7j"] = [0, 0, 0, 0, 0, 0, 0]
                except Exception as e:
                    print(f"‚ö†Ô∏è Erreur calcul jours: {e}")
                    # En cas d'erreur, r√©initialiser l'historique
                    analytics["historique_7j"] = [0, 0, 0, 0, 0, 0, 0]
            
            # R√©initialiser le compteur du jour actuel
            analytics["visiteurs_aujourdhui"] = 0
            analytics["derniere_date"] = date_actuelle
        
        # Incr√©menter les compteurs
        analytics["visiteurs_totaux"] = analytics.get("visiteurs_totaux", 0) + 1
        analytics["visiteurs_en_ligne"] = analytics.get("visiteurs_en_ligne", 0) + 1
        analytics["visiteurs_aujourdhui"] = analytics.get("visiteurs_aujourdhui", 0) + 1
        
        # Mettre √† jour l'historique des 7 derniers jours (dernier √©l√©ment = aujourd'hui)
        if len(analytics["historique_7j"]) > 0:
            analytics["historique_7j"][-1] = analytics["visiteurs_aujourdhui"]
        else:
            analytics["historique_7j"] = [analytics["visiteurs_aujourdhui"]]
        
        # Mettre √† jour le pic de connexions si n√©cessaire
        if analytics["visiteurs_en_ligne"] > analytics.get("pic_connexions", 0):
            analytics["pic_connexions"] = analytics["visiteurs_en_ligne"]
        
        # Sauvegarder
        with open(ANALYTICS_FILE, 'w', encoding='utf-8') as f:
            json.dump(analytics, f, ensure_ascii=False, indent=2)
        
        return jsonify({"success": True})
    except Exception as e:
        print(f"‚ùå Erreur tracking visiteur: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500

@app.route('/api/detections-history', methods=['GET'])
def api_get_detections_history():
    """Retourne l'historique des d√©tections PMR"""
    try:
        historique = charger_historique_detections()
        # Filtrer par match si sp√©cifi√©
        match_filter = request.args.get('match')
        if match_filter:
            historique = [d for d in historique if match_filter.lower() in d.get('match', '').lower()]
        return jsonify(historique)
    except Exception as e:
        log(f"‚ùå Erreur r√©cup√©ration historique: {e}", 'error')
        return jsonify({"error": str(e)}), 500

@app.route('/api/analytics/telegram-click', methods=['POST'])
def api_track_telegram_click():
    """Enregistre un clic sur le bouton Telegram"""
    try:
        # Charger analytics
        try:
            with open(ANALYTICS_FILE, 'r', encoding='utf-8') as f:
                analytics = json.load(f)
        except FileNotFoundError:
            # Initialiser avec toutes les propri√©t√©s n√©cessaires
            analytics = {
                "visiteurs_totaux": 0,
                "visiteurs_en_ligne": 0,
                "visiteurs_aujourdhui": 0,
                "temps_moyen": "0m 0s",
                "taux_rebond": "0%",
                "clics_telegram": 0,
                "pic_connexions": 0,
                "taux_retour": "0%",
                "historique_7j": [0, 0, 0, 0, 0, 0, 0]
            }
        
        # S'assurer que toutes les propri√©t√©s existent
        if "clics_telegram" not in analytics:
            analytics["clics_telegram"] = 0
        
        # Incr√©menter
        analytics["clics_telegram"] = analytics.get("clics_telegram", 0) + 1
        
        # Sauvegarder
        with open(ANALYTICS_FILE, 'w', encoding='utf-8') as f:
            json.dump(analytics, f, ensure_ascii=False, indent=2)
        
        return jsonify({"success": True})
    except Exception as e:
        print(f"‚ùå Erreur tracking clic Telegram: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500

@app.route('/api/logs', methods=['GET'])
def api_get_logs():
    """Retourne les logs du backend"""
    try:
        limit = request.args.get('limit', 50, type=int)
        logs = list(backend_logs)[-limit:]  # Derniers N logs
        return jsonify({
            "success": True,
            "logs": logs,
            "total": len(backend_logs)
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/groq/analyze', methods=['GET'])
def api_groq_analyze():
    """G√©n√®re une analyse IA compl√®te du match avec Groq (analysis, comparison, weather, lineups)"""
    try:
        match_name = request.args.get('match')
        if not match_name:
            return jsonify({"error": "Param√®tre 'match' requis"}), 400
        
        # V√©rifier le cache d'abord
        cached_data = get_cached_groq_data(match_name)
        if cached_data:
            return jsonify(cached_data)
        
        # Charger les donn√©es du match depuis status.json
        try:
            with open('status.json', 'r', encoding='utf-8') as f:
                status = json.load(f)
        except FileNotFoundError:
            return jsonify({"error": "status.json non trouv√©"}), 404
        
        match = next((m for m in status.get('matchs', []) if m['nom'] == match_name), None)
        if not match:
            return jsonify({"error": "Match non trouv√©"}), 404
        
        # Extraire les √©quipes
        teams = extract_teams_from_match_name(match_name)
        home_team = teams['home']
        away_team = teams['away']
        
        # D√©tecter l'importance
        importance = detect_match_importance(home_team, away_team, match_name)
        
        # R√©cup√©rer les VRAIS matchs de comparaison depuis matches.json
        comparison_matches = get_comparison_matches(match_name, home_team, limit=3)
        
        # Date du match (utiliser date actuelle)
        match_date = datetime.now()
        date_formatted = match_date.strftime("%d %B %Y")
        
        # Construire la section comparaison
        if comparison_matches:
            comparison_section = f"""
2. COMPARAISON AVEC AUTRES MATCHS DE {home_team}:
   
   Analyse ET compare "{match_name}" avec ces matchs R√âELS du calendrier :
   
   {chr(10).join([f'   {i+1}. {m["name"]}' for i, m in enumerate(comparison_matches)])}
   
   Pour CHAQUE match ci-dessus, analyse son importance et donne un score d'anticipation (0-100).
   R√®gles:
   - {home_team}-OM (Le Classique) = toujours le plus haut (90-100)
   - {home_team}-Lyon = tr√®s attractif (80-92)
   - {home_team}-Monaco = attractif (75-88)
   - Autres √©quipes = variable selon classement (60-80)
   
   Compare "{match_name}" avec ces matchs et classe-les par importance.
   Retourne les scores pour:
   - current_match: score de "{match_name}"
   {chr(10).join([f'   - {m["key"]}: score de "{m["name"]}"' for m in comparison_matches])}
"""
        else:
            comparison_section = f"""
2. COMPARAISON AVEC AUTRES MATCHS:
   G√©n√®re des scores estim√©s pour 3 autres matchs importants de {home_team}:
   - match_1: {home_team} vs Lyon (grand rival)
   - match_2: {home_team} vs Monaco (affiche attractive)
   - match_3: {home_team} vs Lens (match moyen)
"""
        
        # Construire les parties du prompt qui contiennent des backslashes
        psg_lineup_text = '- Utilise les vrais joueurs du PSG actuel: Donnarumma (GK), Hakimi, Marquinhos (C), Skriniar, Mendes (DF), Vitinha, Za√Øre-Emery, Ugarte (MF), Demb√©l√©, Ramos, Barcola (FW)'
        om_lineup_text = "- Utilise les vrais joueurs de l'OM actuel: L√≥pez (GK), Clauss, Gigot, Balerdi, Tavares (DF), Rongier, Veretout, Harit (MF), Aubameyang, Greenwood, Moumbagna (FW)"
        
        home_lineup_instruction = psg_lineup_text if home_team == 'PSG' else f'- Utilise les vrais joueurs actuels de {home_team}'
        away_lineup_instruction = om_lineup_text if ('OM' in away_team or 'Marseille' in away_team) else f'- Utilise les vrais joueurs actuels de {away_team}'
        
        # Construire la partie comparaison (sans backslash dans les expressions)
        comparison_json_lines = []
        comparison_name_lines = []
        if comparison_matches:
            for m in comparison_matches:
                comparison_json_lines.append(f'    "{m["key"]}": number,')
                comparison_name_lines.append(f'    "{m["key"]}_name": "{m["name"]}",')
        else:
            comparison_json_lines = ['    "match_1": number,', '    "match_2": number,', '    "match_3": number,']
        
        # Construire les cha√Ænes de comparaison
        comparison_json_str = '\n'.join(comparison_json_lines)
        comparison_name_str = '\n'.join(comparison_name_lines) if comparison_name_lines else ''
        
        # Construire les parties du prompt avec des conditions
        importance_text = 'Match √† tr√®s forte affluence' if importance['is_high_profile'] else 'Match d\'importance moyenne'
        classico_text = 'Le Classique PSG-OM g√©n√®re toujours une demande exceptionnelle (90-100%).' if importance['is_classico'] else ''
        ol_text = 'PSG-OL est une affiche majeure de Ligue 1 (80-95%).' if importance['is_ol'] else ''
        monaco_text = 'PSG-Monaco est un match attractif (75-90%).' if importance['is_monaco'] else ''
        other_text = 'Pour un match moins m√©diatis√©, ajuste les scores en cons√©quence (60-85%).' if not importance['is_high_profile'] else ''
        
        # Construire le template JSON s√©par√©ment
        json_template = """{{
  "analysis": {{
    "hype_score": number,
    "affluence_prevue": number,
    "probabilite_pmr": number,
    "analyse": "string adapt√©e √† """ + match_name + """"
  }},
  "comparison": {{
    "current_match": number,
""" + comparison_json_str + """
""" + comparison_name_str + """
  }},
  "weather": {{
    "temperature": number,
    "condition": "string",
    "rain_chance": number,
    "wind_speed": number,
    "emoji": "string"
  }},
  "lineups": {{
    "home": {{
      "formation": "string",
      "gk": ["string"],
      "df": ["string", "string", "string", "string"],
      "mf": ["string", "string", "string"],
      "fw": ["string", "string", "string"]
    }},
    "away": {{
      "formation": "string",
      "gk": ["string"],
      "df": ["string", "string", "string", "string"],
      "mf": ["string", "string", "string"],
      "fw": ["string", "string", "string"]
    }}
  }}
}}"""
        
        # Construire le prompt complet en concat√©nant les parties
        prompt = f"""Tu es un expert en football fran√ßais, m√©t√©orologie et analyse de donn√©es sportives.

MATCH √Ä ANALYSER:
- √âquipes: {match_name}
- Comp√©tition: Ligue 1
- Date: {date_formatted}
- Stade: Parc des Princes
- Contexte: {importance['rivalry']}
- Importance: {importance_text}
- Nombre de v√©rifications: {match.get('nb_checks', 0)}
- Statut PMR actuel: {'Disponible' if match.get('pmr_disponible', False) else 'Non disponible'}

CONSIGNES D'ANALYSE D√âTAILL√âE:

1. ANALYSE D'ANTICIPATION APPROFONDIE:
   Analyse en profondeur le niveau d'attente pour ce match sp√©cifique "{match_name}".
   {classico_text}
   {ol_text}
   {monaco_text}
   {other_text}
   
   - hype_score: niveau d'anticipation supporters (0-100) - Justifie avec des √©l√©ments concrets
   - affluence_prevue: taux de remplissage estim√© (0-100) - Base-toi sur l'historique du Parc des Princes
   - probabilite_pmr: chance qu'une place PMR se lib√®re (0-100) - Consid√®re la raret√© des places PMR
   - analyse: explication D√âTAILL√âE (5-7 phrases) incluant:
     * Contexte du match (rivalit√©, enjeux, importance)
     * Historique des places PMR pour ce type de match
     * Facteurs influen√ßant la disponibilit√© (demande, timing, saison)
     * Recommandations concr√®tes pour l'utilisateur
     * Probabilit√© d√©taill√©e avec justification

{comparison_section}

3. M√âT√âO PR√âVUE D√âTAILL√âE:
   Pour Parc des Princes le {date_formatted}:
   - Utilise des donn√©es m√©t√©o r√©alistes pour Paris/France √† cette p√©riode
   - En janvier: g√©n√©ralement 5-10¬∞C, souvent nuageux, risque de pluie moyen
   - En √©t√©: 20-30¬∞C, plut√¥t ensoleill√©
   - Adapte selon la saison r√©elle
   
   - temperature: temp√©rature en ¬∞C (coh√©rente avec la date)
   - condition: description d√©taill√©e ("Ensoleill√© avec quelques nuages", "Nuageux avec averses possibles", etc.)
   - rain_chance: probabilit√© de pluie (0-100) avec justification
   - wind_speed: vitesse vent en km/h (10-20 km/h typique)
   - emoji: emoji m√©t√©o appropri√© (‚òÄÔ∏è, üå§Ô∏è, ‚õÖ, üåßÔ∏è, ‚õàÔ∏è, etc.)

4. COMPOSITIONS PROBABLES D√âTAILL√âES:
   G√©n√®re les compositions R√âALISTES et ACTUELLES (saison 2024-2025):
   
   Pour {home_team}:
   {home_lineup_instruction}
   - Formation: 4-3-3 (typique) ou autre selon le contexte
   - Inclus les vrais noms de joueurs actuels
   
   Pour {away_team}:
   {away_lineup_instruction}
   - Formation: 4-3-3 ou autre selon le contexte
   - Inclus les vrais noms de joueurs actuels

IMPORTANT:
- Analyse TR√àS D√âTAILL√âE avec justification de chaque score
- Adapte TOUS les scores et analyses au match sp√©cifique "{match_name}"
- Ne copie pas les valeurs d'un autre match
- Sois coh√©rent: PSG-OM > PSG-OL > PSG-Monaco > PSG-√©quipe moyenne
- Utilise les vrais effectifs 2024-2025 avec noms r√©els
- M√©t√©o r√©aliste et d√©taill√©e pour {date_formatted}
- L'analyse doit faire 5-7 phrases minimum avec d√©tails concrets

R√©ponds UNIQUEMENT avec ce JSON, sans texte avant/apr√®s, sans markdown:
""" + json_template

        # Cl√© API Groq (doit √™tre d√©finie dans les variables d'environnement)
        GROQ_API_KEY = os.getenv("GROQ_API_KEY")
        if not GROQ_API_KEY:
            log("‚ö†Ô∏è GROQ_API_KEY non d√©finie, impossible de g√©n√©rer l'analyse", 'warning')
            return jsonify({"error": "GROQ_API_KEY non configur√©e"}), 500
        
        GROQ_API_URL = "https://api.groq.com/openai/v1/chat/completions"
        
        log(f"üì° Appel API Groq pour {match_name}", 'info')
        log(f"üîë GROQ_API_KEY pr√©sente: {'Oui' if GROQ_API_KEY else 'Non'}", 'info')
        log(f"üîó URL API: {GROQ_API_URL}", 'info')
        
        # Appeler l'API Groq
        headers = {
            "Authorization": f"Bearer {GROQ_API_KEY}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": "llama-3.1-70b-versatile",
            "messages": [
                {
                    "role": "system",
                    "content": "Tu es un expert en football fran√ßais, m√©t√©orologie et analyse de donn√©es sportives. R√©ponds UNIQUEMENT avec du JSON valide, sans markdown, sans code blocks."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "temperature": 0.4,
            "max_tokens": 1500,
            "top_p": 0.9
        }
        
        log(f"üì§ Payload envoy√© - Model: {payload['model']}, Messages: {len(payload['messages'])}", 'info')
        log(f"üìù Taille du prompt: {len(prompt)} caract√®res", 'info')
        
        try:
            response = requests.post(GROQ_API_URL, json=payload, headers=headers, timeout=30)
            log(f"üì• R√©ponse Groq re√ßue - Status: {response.status_code}", 'info')
        except requests.exceptions.Timeout:
            log(f"‚è±Ô∏è Timeout lors de l'appel API Groq (30s d√©pass√©)", 'error')
            raise
        except requests.exceptions.RequestException as e:
            log(f"‚ùå Erreur r√©seau lors de l'appel API Groq: {e}", 'error')
            raise
        
        if response.status_code != 200:
            error_detail = ""
            try:
                error_response = response.json()
                error_detail = f" - {error_response.get('error', {}).get('message', str(error_response))}"
            except:
                error_detail = f" - {response.text[:500]}"
            log(f"‚ùå Erreur API Groq: {response.status_code}{error_detail}", 'error')
            log(f"üìÑ R√©ponse compl√®te (premiers 1000 caract√®res): {response.text[:1000]}", 'error')
            # Retourner des donn√©es par d√©faut au lieu d'une erreur 500
            default_data = {
                "analysis": {
                    "hype_score": 75,
                    "affluence_prevue": 85,
                    "probabilite_pmr": 15,
                    "analyse": f"Le match {match_name} a √©t√© v√©rifi√© {match.get('nb_checks', 0)} fois. Bas√© sur l'historique, la probabilit√© de disponibilit√© de places PMR est mod√©r√©e. Recommandation : activer les alertes Telegram pour ne pas manquer une opportunit√©."
                },
                "comparison": {
                    "current_match": 75,
                    "match_1": 70,
                    "match_1_name": comparison_matches[0]['name'] if comparison_matches else f"{home_team} vs Lyon",
                    "match_2": 65,
                    "match_2_name": comparison_matches[1]['name'] if len(comparison_matches) > 1 else f"{home_team} vs Monaco",
                    "match_3": 60,
                    "match_3_name": comparison_matches[2]['name'] if len(comparison_matches) > 2 else f"{home_team} vs Lens"
                },
                "weather": {
                    "temperature": 12,
                    "condition": "Variable",
                    "rain_chance": 30,
                    "wind_speed": 15,
                    "emoji": "üå§Ô∏è"
                },
                "lineups": {
                    "home": {
                        "formation": "4-3-3",
                        "gk": ["Gardien"],
                        "df": ["DF1", "DF2", "DF3", "DF4"],
                        "mf": ["MF1", "MF2", "MF3"],
                        "fw": ["FW1", "FW2", "FW3"]
                    },
                    "away": {
                        "formation": "4-3-3",
                        "gk": ["Gardien"],
                        "df": ["DF1", "DF2", "DF3", "DF4"],
                        "mf": ["MF1", "MF2", "MF3"],
                        "fw": ["FW1", "FW2", "FW3"]
                    }
                },
                "last_updated": datetime.now().isoformat(),
                "error": True
            }
            save_groq_cache(match_name, default_data)
            return jsonify(default_data)
        
        result = response.json()
        log(f"‚úÖ R√©ponse JSON pars√©e avec succ√®s", 'info')
        log(f"üìä Nombre de choix: {len(result.get('choices', []))}", 'info')
        
        if 'choices' not in result or len(result['choices']) == 0:
            raise ValueError("Aucun choix dans la r√©ponse Groq")
        
        content = result['choices'][0]['message']['content']
        content_original = content  # Sauvegarder pour les logs d'erreur
        log(f"üìù Contenu brut re√ßu (premiers 500 caract√®res): {content[:500]}", 'info')
        log(f"üìè Taille du contenu: {len(content)} caract√®res", 'info')
        
        # Parser le JSON de la r√©ponse
        try:
            # Nettoyer le contenu (enlever markdown si pr√©sent)
            content = content.strip()
            if content.startswith('```json'):
                content = content[7:]
            if content.startswith('```'):
                content = content[3:]
            if content.endswith('```'):
                content = content[:-3]
            content = content.strip()
            
            # Extraire le JSON
            json_match = None
            if '{' in content:
                start = content.find('{')
                end = content.rfind('}') + 1
                json_match = content[start:end]
            
            if not json_match:
                raise ValueError("Aucun JSON trouv√© dans la r√©ponse")
            
            complete_data = json.loads(json_match)
            
            # V√©rifier que toutes les sections sont pr√©sentes
            if not all(key in complete_data for key in ['analysis', 'comparison', 'weather', 'lineups']):
                raise ValueError("Donn√©es incompl√®tes dans la r√©ponse Groq")
            
            # Ajouter timestamp
            complete_data['last_updated'] = datetime.now().isoformat()
            
            # Logger la r√©ponse Groq compl√®te de mani√®re structur√©e
            log(f"‚úÖ R√©ponse Groq re√ßue pour {match_name}", 'info')
            
            # Logger chaque section s√©par√©ment pour plus de lisibilit√©
            if 'analysis' in complete_data:
                analysis = complete_data['analysis']
                log(f"üìä Analyse IA - Hype: {analysis.get('hype_score', 'N/A')}% | Affluence: {analysis.get('affluence_prevue', 'N/A')}% | Probabilit√© PMR: {analysis.get('probabilite_pmr', 'N/A')}%", 'info')
                log(f"üí≠ Analyse d√©taill√©e: {analysis.get('analyse', 'N/A')[:200]}...", 'info')
            
            if 'comparison' in complete_data:
                comp = complete_data['comparison']
                log(f"üìà Comparaison - Match actuel: {comp.get('current_match', 'N/A')}%", 'info')
            
            if 'weather' in complete_data:
                weather = complete_data['weather']
                log(f"üå§Ô∏è M√©t√©o - {weather.get('temperature', 'N/A')}¬∞C | {weather.get('condition', 'N/A')} | Pluie: {weather.get('rain_chance', 'N/A')}% | Vent: {weather.get('wind_speed', 'N/A')} km/h", 'info')
            
            if 'lineups' in complete_data:
                lineups = complete_data['lineups']
                home_form = lineups.get('home', {}).get('formation', 'N/A')
                away_form = lineups.get('away', {}).get('formation', 'N/A')
                log(f"‚öΩ Compositions - Domicile: {home_form} | Ext√©rieur: {away_form}", 'info')
            
            # Logger le JSON complet pour r√©f√©rence (format√©)
            log(f"üìã JSON Groq complet:\n{json.dumps(complete_data, ensure_ascii=False, indent=2)}", 'info')
            
            # Sauvegarder dans le cache
            save_groq_cache(match_name, complete_data)
            
            return jsonify(complete_data)
            
        except (json.JSONDecodeError, ValueError) as e:
            # Si le parsing √©choue, retourner des donn√©es par d√©faut
            log(f"‚ö†Ô∏è R√©ponse Groq invalide, utilisation de valeurs par d√©faut: {e}", 'warning')
            log(f"üìÑ Contenu original (premiers 1000 caract√®res): {content_original[:1000]}", 'warning')
            log(f"üìÑ Contenu nettoy√© (premiers 1000 caract√®res): {content[:1000]}", 'warning')
            if json_match:
                log(f"üìÑ JSON extrait (premiers 1000 caract√®res): {json_match[:1000]}", 'warning')
            default_data = {
                "analysis": {
                    "hype_score": 75,
                    "affluence_prevue": 85,
                    "probabilite_pmr": 15,
                    "analyse": f"Le match {match_name} a √©t√© v√©rifi√© {match.get('nb_checks', 0)} fois. Bas√© sur l'historique, la probabilit√© de disponibilit√© de places PMR est mod√©r√©e. Recommandation : activer les alertes Telegram pour ne pas manquer une opportunit√©."
                },
                "comparison": {
                    "current_match": 75,
                    "match_1": 70,
                    "match_1_name": comparison_matches[0]['name'] if comparison_matches else f"{home_team} vs Lyon",
                    "match_2": 65,
                    "match_2_name": comparison_matches[1]['name'] if len(comparison_matches) > 1 else f"{home_team} vs Monaco",
                    "match_3": 60,
                    "match_3_name": comparison_matches[2]['name'] if len(comparison_matches) > 2 else f"{home_team} vs Lens"
                },
                "weather": {
                    "temperature": 12,
                    "condition": "Variable",
                    "rain_chance": 30,
                    "wind_speed": 15,
                    "emoji": "üå§Ô∏è"
                },
                "lineups": {
                    "home": {
                        "formation": "4-3-3",
                        "gk": ["Gardien"],
                        "df": ["DF1", "DF2", "DF3", "DF4"],
                        "mf": ["MF1", "MF2", "MF3"],
                        "fw": ["FW1", "FW2", "FW3"]
                    },
                    "away": {
                        "formation": "4-3-3",
                        "gk": ["Gardien"],
                        "df": ["DF1", "DF2", "DF3", "DF4"],
                        "mf": ["MF1", "MF2", "MF3"],
                        "fw": ["FW1", "FW2", "FW3"]
                    }
                },
                "last_updated": datetime.now().isoformat(),
                "error": True
            }
            save_groq_cache(match_name, default_data)
            return jsonify(default_data)
            
    except Exception as e:
        log(f"‚ùå Erreur analyse Groq: {e}", 'error')
        import traceback
        traceback.print_exc()
        # Retourner des donn√©es par d√©faut au lieu d'une erreur 500
        try:
            match_name = request.args.get('match', 'Match inconnu')
            teams = extract_teams_from_match_name(match_name)
            home_team = teams['home']
            comparison_matches = get_comparison_matches(match_name, home_team, limit=3)
        except:
            home_team = 'PSG'
            comparison_matches = []
        
        default_data = {
            "analysis": {
                "hype_score": 75,
                "affluence_prevue": 85,
                "probabilite_pmr": 15,
                "analyse": f"Erreur lors de la g√©n√©ration de l'analyse IA. Donn√©es par d√©faut affich√©es."
            },
            "comparison": {
                "current_match": 75,
                "match_1": 70,
                "match_1_name": comparison_matches[0]['name'] if comparison_matches else f"{home_team} vs Lyon",
                "match_2": 65,
                "match_2_name": comparison_matches[1]['name'] if len(comparison_matches) > 1 else f"{home_team} vs Monaco",
                "match_3": 60,
                "match_3_name": comparison_matches[2]['name'] if len(comparison_matches) > 2 else f"{home_team} vs Lens"
            },
            "weather": {
                "temperature": 12,
                "condition": "Variable",
                "rain_chance": 30,
                "wind_speed": 15,
                "emoji": "üå§Ô∏è"
            },
            "lineups": {
                "home": {
                    "formation": "4-3-3",
                    "gk": ["Gardien"],
                    "df": ["DF1", "DF2", "DF3", "DF4"],
                    "mf": ["MF1", "MF2", "MF3"],
                    "fw": ["FW1", "FW2", "FW3"]
                },
                "away": {
                    "formation": "4-3-3",
                    "gk": ["Gardien"],
                    "df": ["DF1", "DF2", "DF3", "DF4"],
                    "mf": ["MF1", "MF2", "MF3"],
                    "fw": ["FW1", "FW2", "FW3"]
                }
            },
            "last_updated": datetime.now().isoformat(),
            "error": True
        }
        return jsonify(default_data)

def start_flask_api():
    """D√©marre l'API Flask dans un thread s√©par√©"""
    app.run(host='0.0.0.0', port=5000, debug=False, use_reloader=False)

# D√©marrer l'API Flask en arri√®re-plan
threading.Thread(target=start_flask_api, daemon=True).start()
log("üîå API Flask d√©marr√©e sur le port 5000", 'success')

# D√©marrer le serveur web dans un thread s√©par√©
def start_web_server():
    """Serveur web simple pour servir index.html et status.json"""
    port = 8081  # Port diff√©rent du site pour √©viter les conflits
    
    class CustomHandler(SimpleHTTPRequestHandler):
        def __init__(self, *args, **kwargs):
            super().__init__(*args, directory='Site', **kwargs)
        
        def end_headers(self):
            # Ajouter les headers CORS pour permettre l'acc√®s depuis n'importe o√π
            self.send_header('Access-Control-Allow-Origin', '*')
            self.send_header('Access-Control-Allow-Methods', 'GET, POST, OPTIONS, DELETE')
            self.send_header('Access-Control-Allow-Headers', '*')
            super().end_headers()
        
        def do_OPTIONS(self):
            """G√©rer les requ√™tes OPTIONS pour CORS"""
            self.send_response(200)
            self.end_headers()
        
        def _proxy_to_flask(self, method='GET'):
            """Proxy les requ√™tes /api/* vers Flask sur le port 5000"""
            import urllib.request
            import urllib.parse
            
            try:
                # Construire l'URL Flask
                flask_url = f'http://localhost:5000{self.path}'
                
                # Pr√©parer la requ√™te
                req_data = None
                if method == 'POST' or method == 'PUT' or method == 'DELETE':
                    content_length = int(self.headers.get('Content-Length', 0))
                    if content_length > 0:
                        req_data = self.rfile.read(content_length)
                
                # Cr√©er la requ√™te
                req = urllib.request.Request(flask_url, data=req_data, method=method)
                
                # Copier les headers
                for header, value in self.headers.items():
                    if header.lower() not in ['host', 'content-length']:
                        req.add_header(header, value)
                
                # Faire la requ√™te
                with urllib.request.urlopen(req, timeout=10) as response:
                    # Envoyer la r√©ponse
                    self.send_response(response.getcode())
                    # Copier les headers de Flask SAUF les headers CORS (on les g√®re nous-m√™mes)
                    for header, value in response.headers.items():
                        header_lower = header.lower()
                        if header_lower not in ['connection', 'transfer-encoding', 
                                                'access-control-allow-origin', 
                                                'access-control-allow-methods',
                                                'access-control-allow-headers',
                                                'access-control-allow-credentials']:
                            self.send_header(header, value)
                    # Les headers CORS seront ajout√©s par end_headers()
                    self.end_headers()
                    self.wfile.write(response.read())
                    
            except Exception as e:
                print(f"‚ùå Erreur proxy Flask: {e}")
                self.send_response(502)
                self.send_header('Content-type', 'application/json')
                self.end_headers()
                error_msg = json.dumps({"error": f"Proxy error: {str(e)}"})
                self.wfile.write(error_msg.encode('utf-8'))
        
        def do_GET(self):
            # Si c'est une requ√™te API, proxy vers Flask
            if self.path.startswith('/api/'):
                self._proxy_to_flask('GET')
                return
            
            # Si on demande status.json, le servir depuis la racine du projet
            if self.path == '/status.json' or self.path == '/status.json/':
                import os
                # status.json est dans le WORKDIR (/app)
                # Utiliser le chemin absolu depuis le r√©pertoire de travail
                status_path = os.path.join(os.getcwd(), 'status.json')
                print(f"üîç Tentative de servir status.json depuis: {status_path}")
                print(f"üîç Fichier existe: {os.path.exists(status_path)}")
                
                if os.path.exists(status_path):
                    self.send_response(200)
                    self.send_header('Content-type', 'application/json')
                    self.end_headers()
                    with open(status_path, 'rb') as f:
                        self.wfile.write(f.read())
                    print(f"‚úÖ status.json servi avec succ√®s")
                    return
                else:
                    # Essayer aussi /app/status.json au cas o√π
                    alt_path = '/app/status.json'
                    if os.path.exists(alt_path):
                        self.send_response(200)
                        self.send_header('Content-type', 'application/json')
                        self.end_headers()
                        with open(alt_path, 'rb') as f:
                            self.wfile.write(f.read())
                        print(f"‚úÖ status.json servi depuis {alt_path}")
                        return
                    else:
                        self.send_response(404)
                        self.send_header('Content-type', 'application/json')
                        self.end_headers()
                        error_msg = json.dumps({"error": "status.json not found", "cwd": os.getcwd(), "paths_checked": [status_path, alt_path]})
                        self.wfile.write(error_msg.encode('utf-8'))
                        print(f"‚ùå status.json non trouv√©. CWD: {os.getcwd()}")
                        return
            
            # G√©rer les routes sans extension (comme /admin)
            if self.path == '/admin' or self.path == '/admin/':
                self.path = '/admin.html'
            
            # Sinon, servir depuis le dossier Site
            return super().do_GET()
        
        def do_POST(self):
            # Si c'est une requ√™te API, proxy vers Flask
            if self.path.startswith('/api/'):
                self._proxy_to_flask('POST')
                return
            
            # Sinon, 404
            self.send_response(404)
            self.send_header('Content-type', 'application/json')
            self.end_headers()
            error_msg = json.dumps({"error": "Not found"})
            self.wfile.write(error_msg.encode('utf-8'))
        
        def do_DELETE(self):
            # Si c'est une requ√™te API, proxy vers Flask
            if self.path.startswith('/api/'):
                self._proxy_to_flask('DELETE')
                return
            
            # Sinon, 404
            self.send_response(404)
            self.send_header('Content-type', 'application/json')
            self.end_headers()
            error_msg = json.dumps({"error": "Not found"})
            self.wfile.write(error_msg.encode('utf-8'))
        
        def log_message(self, format, *args):
            # R√©duire les logs verbeux
            pass
    
    server = HTTPServer(('0.0.0.0', port), CustomHandler)
    log(f"üåê Serveur web d√©marr√© sur le port {port}", 'success')
    log(f"üì± Site accessible sur http://localhost:{port}/index.html", 'info')
    server.serve_forever()

# Lancer le serveur web en arri√®re-plan
threading.Thread(target=start_web_server, daemon=True).start()

log("üöÄ Bot PSM d√©marr√© avec serveur web int√©gr√©!", 'success')

# ‚úÖ BOUCLE PRINCIPALE MULTI-MATCHS
while True:
    MATCHS = charger_matchs()  # Recharger les matchs √† chaque it√©ration
    log(f"üìã Cycle de surveillance: {len(MATCHS)} match(s) √† v√©rifier", 'info')
    if len(MATCHS) > 0:
        matchs_noms = ', '.join([m['nom'] for m in MATCHS])
        log(f"üìù Matchs: {matchs_noms}", 'info')
    for match in MATCHS:
        verifier_match(match)

    pause = 90 + random.randint(0, 5)
    log(f"‚è≥ Pause {pause} secondes...", 'info')
    time.sleep(pause)


